{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVEC5DybVbYA"
      },
      "source": [
        "In this file we develop a Neural Network from scratch, and implement its backpropagtion algorithm just using mathematical libraries of numpy.\n",
        "\n",
        "The purpose of the netwoork is to acquire a deeper inside into backpropagation.\n",
        "The code in this notebook tightly reflects the pseudocode given in the slides."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWRbCCxaGxtN"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(precision=2, suppress=True)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7sxnOVPHAgw"
      },
      "source": [
        "Let us define a couple of activation functions (sigmoid and relu) and their derivatives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT4CbsndH_kX"
      },
      "source": [
        "##############################################\n",
        "# activation functions\n",
        "##############################################\n",
        "\n",
        "def sigmoid(x): return 1 / (1 + math.exp(-x))\n",
        "\n",
        "def sigderiv(x): return (sigmoid(x)*(1-sigmoid(x)))\n",
        "\n",
        "def relu(x):\n",
        "  if x >= 0: return x\n",
        "  else: return 0\n",
        "\n",
        "def reluderiv(x):\n",
        "  if x >= 0: return 1\n",
        "  else: return 0\n",
        "\n",
        "def activate(x): return sigmoid(x)  #relu(x)\n",
        "def actderiv(x): return sigderiv(x) #reluderiv(x)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Saa8htemIY7T"
      },
      "source": [
        "A neural network is just a collection of numerical vectors describing the weigths of the links at each layer. For instance, a dense layer between n input neurons and m output neurons is defined by a matrix w of dimension nxm for the weights and a vector b of dimension m for the biases.\n",
        "\n",
        "Supposing the network is dense, its architecture is fullly specified by the number of neurons at each layer. For our example, we define a shallow network with 8 input neurons,\n",
        "3 hidden neurons, and 8 output neurons, hence with dimension [8,3,8].\n",
        "\n",
        "We initialize weights and biases with random values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqjjvojNME14"
      },
      "source": [
        "##############################################\n",
        "# net parameters\n",
        "##############################################\n",
        "\n",
        "dim = [8,3,8]\n",
        "l = len(dim)\n",
        "\n",
        "w,b = [],[]\n",
        "\n",
        "for i in range(1,l):\n",
        "  w.append(np.random.rand(dim[i-1],dim[i]))\n",
        "  b.append(np.random.rand(dim[i]))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFhkL6jRM4q8"
      },
      "source": [
        "For the backpropagation algorithm we also need to compute, at each layer, the weighted sum z (inputs to activation), the activation a, and the partial derivative d of the error relative to z.\n",
        "\n",
        "We define a version of the backpropagation algorithm working \"on line\", processing a single training sample (x,y) at a time, and updating the nework parameters at each iteration. The backpropagation function also return the current error  relative to (x,y).\n",
        "\n",
        "An epoch, is a full pass of the error update on all training data; it returns the cumulative error on all data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "468aM7K-OJs1"
      },
      "source": [
        "##############################################\n",
        "# training - on line, one input data at a time\n",
        "##############################################\n",
        "\n",
        "mu = 1\n",
        "\n",
        "z,a,d=[],[],[]\n",
        "\n",
        "for i in range(0,l):\n",
        "  a.append(np.zeros(dim[i]))\n",
        "\n",
        "for i in range(1,l):\n",
        "  z.append(np.zeros(dim[i]))\n",
        "  d.append(np.zeros(dim[i]))\n",
        "\n",
        "def update(x,y):\n",
        "  #input\n",
        "  a[0] = x\n",
        "  #feed forward\n",
        "  for i in range(0,l-1):\n",
        "    z[i] = np.dot(a[i],w[i])+b[i]\n",
        "    a[i+1] = np.vectorize(activate)(z[i])\n",
        "  #output error\n",
        "  d[l-2] = (y - a[l-1])*np.vectorize(actderiv)(z[l-2])\n",
        "  #back propagation\n",
        "  for i in range(l-3,-1,-1):\n",
        "    d[i]=np.dot(w[i+1],d[i+1])*np.vectorize(actderiv)(z[i])\n",
        "  #updating\n",
        "  for i in range(0,l-1):\n",
        "    for k in range (0,dim[i+1]):\n",
        "      for j in range (0,dim[i]):\n",
        "        w[i][j,k] = w[i][j,k] + mu*a[i][j]*d[i][k]\n",
        "      b[i][k] = b[i][k] + mu*d[i][k]\n",
        "    if False:\n",
        "      print(\"d[%i] = %s\" % (i,(d[i],)))\n",
        "      print(\"b[%i] = %s\" % (i,(b[i],)))\n",
        "  #print(\"error = {}\".format(np.sum((y-a[l-1])**2)))\n",
        "  return np.sum((y-a[l-1])**2)\n",
        "\n",
        "def epoch(data):\n",
        "    e = 0\n",
        "    for (x,y) in data:\n",
        "      e += update(x,y)\n",
        "    return e"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO242-WAQCY1"
      },
      "source": [
        "Now we define same data and fit the network over them.\n",
        "\n",
        "We want to define a simple example of autoencoder, taking in input a one-hot representation of the numbers between 0 and 7, and trying to compress them to a\n",
        "boolean internal representation on 3 bits.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2M52bfsFXpp",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d681c9d1-e38d-49c9-e411-8d1664bd0f89"
      },
      "source": [
        "X = [[1,0,0,0,0,0,0,0],\n",
        "     [0,1,0,0,0,0,0,0],\n",
        "     [0,0,1,0,0,0,0,0],\n",
        "     [0,0,0,1,0,0,0,0],\n",
        "     [0,0,0,0,1,0,0,0],\n",
        "     [0,0,0,0,0,1,0,0],\n",
        "     [0,0,0,0,0,0,1,0],\n",
        "     [0,0,0,0,0,0,0,1]]\n",
        "\n",
        "def data(): return zip(X,X)\n",
        "\n",
        "final_error = .003\n",
        "dist = epoch(data())\n",
        "\n",
        "while dist > final_error:\n",
        "  print(\"distance= %f\" % dist)\n",
        "  dist = epoch(data())\n",
        "\n",
        "print(\"distance= %f\" % dist)\n",
        "for x in X:\n",
        "  print(\"input = %s\" % (x,))\n",
        "  a[0] = x\n",
        "  #feed forward\n",
        "  for i in range(0,l-2):\n",
        "    z[i] = np.dot(a[i],w[i])+b[i]\n",
        "    a[i+1] = np.vectorize(activate)(z[i])\n",
        "  print(\"hidden level = %s\" % (a[i+1],))\n",
        "  z[l-2] = np.dot(a[l-2],w[l-2])+b[l-2]\n",
        "  a[l-1] = np.vectorize(activate)(z[l-2])\n",
        "  #print(\"output = %s\" % (a[l-1],))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distance= 0.003000\n",
            "input = [1, 0, 0, 0, 0, 0, 0, 0]\n",
            "hidden level = [0.01 0.01 0.01]\n",
            "input = [0, 1, 0, 0, 0, 0, 0, 0]\n",
            "hidden level = [0.99 0.99 1.  ]\n",
            "input = [0, 0, 1, 0, 0, 0, 0, 0]\n",
            "hidden level = [0.   0.98 0.98]\n",
            "input = [0, 0, 0, 1, 0, 0, 0, 0]\n",
            "hidden level = [0.01 0.96 0.01]\n",
            "input = [0, 0, 0, 0, 1, 0, 0, 0]\n",
            "hidden level = [0.98 0.99 0.01]\n",
            "input = [0, 0, 0, 0, 0, 1, 0, 0]\n",
            "hidden level = [0.98 0.   0.97]\n",
            "input = [0, 0, 0, 0, 0, 0, 1, 0]\n",
            "hidden level = [0.97 0.02 0.01]\n",
            "input = [0, 0, 0, 0, 0, 0, 0, 1]\n",
            "hidden level = [0.01 0.01 0.91]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should interpret the latent representation as a binary encoding:\n",
        "\n"
      ],
      "metadata": {
        "id": "QjbS_Mz5WubH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d681c9d1-e38d-49c9-e411-8d1664bd0f89",
        "id": "EgpmePnqXXri"
      },
      "source": [
        "X = [[1,0,0,0,0,0,0,0],\n",
        "     [0,1,0,0,0,0,0,0],\n",
        "     [0,0,1,0,0,0,0,0],\n",
        "     [0,0,0,1,0,0,0,0],\n",
        "     [0,0,0,0,1,0,0,0],\n",
        "     [0,0,0,0,0,1,0,0],\n",
        "     [0,0,0,0,0,0,1,0],\n",
        "     [0,0,0,0,0,0,0,1]]\n",
        "\n",
        "def data(): return zip(X,X)\n",
        "\n",
        "final_error = .003\n",
        "dist = epoch(data())\n",
        "\n",
        "while dist > final_error:\n",
        "  print(\"distance= %f\" % dist)\n",
        "  dist = epoch(data())\n",
        "\n",
        "print(\"distance= %f\" % dist)\n",
        "for x in X:\n",
        "  print(\"input = %s\" % (x,))\n",
        "  a[0] = x\n",
        "  #feed forward\n",
        "  for i in range(0,l-2):\n",
        "    z[i] = np.dot(a[i],w[i])+b[i]\n",
        "    a[i+1] = np.vectorize(activate)(z[i])\n",
        "  print(\"hidden level = %s\" % (a[i+1],))\n",
        "  z[l-2] = np.dot(a[l-2],w[l-2])+b[l-2]\n",
        "  a[l-1] = np.vectorize(activate)(z[l-2])\n",
        "  #print(\"output = %s\" % (a[l-1],))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distance= 0.003000\n",
            "input = [1, 0, 0, 0, 0, 0, 0, 0]\n",
            "hidden level = [0.01 0.01 0.01]\n",
            "input = [0, 1, 0, 0, 0, 0, 0, 0]\n",
            "hidden level = [0.99 0.99 1.  ]\n",
            "input = [0, 0, 1, 0, 0, 0, 0, 0]\n",
            "hidden level = [0.   0.98 0.98]\n",
            "input = [0, 0, 0, 1, 0, 0, 0, 0]\n",
            "hidden level = [0.01 0.96 0.01]\n",
            "input = [0, 0, 0, 0, 1, 0, 0, 0]\n",
            "hidden level = [0.98 0.99 0.01]\n",
            "input = [0, 0, 0, 0, 0, 1, 0, 0]\n",
            "hidden level = [0.98 0.   0.97]\n",
            "input = [0, 0, 0, 0, 0, 0, 1, 0]\n",
            "hidden level = [0.97 0.02 0.01]\n",
            "input = [0, 0, 0, 0, 0, 0, 0, 1]\n",
            "hidden level = [0.01 0.01 0.91]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may interpret the latent representation as a sort of binat encoding: the network is learning binarization!\n",
        "\n",
        "Latent [0.01 0.01 0.01]  ->  0 0 0\n",
        "\n",
        "Latent [0.99 0.99 1.00]    ->  1 1 1\n",
        "\n",
        "Latent [0.00   0.98 0.98]  ->  0 1 1\n",
        "\n",
        "Latent [0.01 0.96 0.01]  ->  0 1 0\n",
        "\n",
        "Latent [0.98 0.99 0.01]  ->  1 1 0\n",
        "\n",
        "Latent [0.98 0.00   0.97]  ->  1 0 1\n",
        "\n",
        "Latent [0.97 0.02 0.01]  ->  1 0 0\n",
        "\n",
        "Latent [0.01 0.01 0.91]  ->  0 0 1"
      ],
      "metadata": {
        "id": "ALn4wWWgXePh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0LkDAsuo6rD"
      },
      "source": [
        "Exercises.\n",
        "\n",
        "1.   change the specification of the network to allow a different activation function for each layer;\n",
        "2.   modify the backpropagation algorithm to work on a minibatch of samples.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}