{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElNaMbLnRdHR"
      },
      "source": [
        "# Sentence Reconstruction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXr4iGUGRms8"
      },
      "source": [
        "The purpose of this project is to take in input a sequence of words corresponding to a random permutation of a given english sentence, and reconstruct the original sentence.\n",
        "\n",
        "The otuput can be either produced in a single shot, or through an iterative (autoregressive) loop generating a single token at a time.\n",
        "\n",
        "\n",
        "CONSTRAINTS:\n",
        "* No pretrained model can be used.\n",
        "* The neural network models should have less the 20M parameters.\n",
        "* No postprocessing should be done (e.g. no beamsearch)\n",
        "* You cannot use additional training data.\n",
        "\n",
        "\n",
        "BONUS PARAMETERS:\n",
        "\n",
        "A bonus of 0-2 points will be attributed to incentivate the adoption of models with a low number of parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ8k-L-WUK7l"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "The dataset is composed by sentences taken from the generics_kb dataset of hugging face. We restricted the vocabolary to the 10K most frequent words, and only took sentences making use of this vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nJ02vehGYySk",
        "outputId": "49ec9576-86cd-44e7-ea52-45d117028658"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the dataset"
      ],
      "metadata": {
        "id": "807Wk-ir_bDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from keras.layers import TextVectorization\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "ds = load_dataset('generics_kb',trust_remote_code=True)['train']"
      ],
      "metadata": {
        "id": "_WjtqA8TrHcS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter row with length greater than 8.\n"
      ],
      "metadata": {
        "id": "lAVLfsdc_ej5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = ds.filter(lambda row: len(row[\"generic_sentence\"].split(\" \")) > 8 )\n",
        "corpus = [ '<start> ' + row['generic_sentence'].replace(\",\",\" <comma>\") + ' <end>' for row in ds ]\n",
        "corpus = np.array(corpus)\n"
      ],
      "metadata": {
        "id": "iznq8xGNt2Zr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a tokenizer and Detokenizer"
      ],
      "metadata": {
        "id": "FyYpXLCF_ldR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=TextVectorization( max_tokens=10000, standardize=\"lower_and_strip_punctuation\", encoding=\"utf-8\",) #con il max prende le piu frequenti. ordina i token del vocab dal piu frequente al meno frequente\n",
        "tokenizer.adapt(corpus)\n",
        "\n",
        "class TextDetokenizer:\n",
        "    def __init__(self, vectorize_layer):\n",
        "        self.vectorize_layer = vectorize_layer\n",
        "        vocab = self.vectorize_layer.get_vocabulary()\n",
        "        self.index_to_word = {index: word for index, word in enumerate(vocab)}\n",
        "\n",
        "    def __detokenize_tokens(self, tokens):\n",
        "        def check_token(t):\n",
        "          if t == 3:\n",
        "            s=\"<start>\"\n",
        "          elif t == 2:\n",
        "            s=\"<end>\"\n",
        "          elif t == 7:\n",
        "            s=\"<comma>\"\n",
        "          else:\n",
        "            s=self.index_to_word.get(t, '[UNK]')\n",
        "          return s\n",
        "\n",
        "        return ' '.join([ check_token(token) for token in tokens if token != 0])\n",
        "\n",
        "    def __call__(self, batch_tokens):\n",
        "       return [self.__detokenize_tokens(tokens) for tokens in batch_tokens]\n",
        "\n",
        "\n",
        "detokenizer = TextDetokenizer( tokenizer )\n",
        "sentences = tokenizer( corpus ).numpy()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "T-bE2JpVbU9E"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove from corpus the sentences where any unknow word appears"
      ],
      "metadata": {
        "id": "lZ64sns1_pSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = np.sum( (sentences==1), axis=1) >= 1\n",
        "original_data = np.delete( sentences, mask , axis=0)"
      ],
      "metadata": {
        "id": "2LPQtryQz5wh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYfOscVk7U0r",
        "outputId": "4adeb4a1-efdf-4d8f-e4dd-2015e9818624"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(241236, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shuffle the sentences"
      ],
      "metadata": {
        "id": "5puiiQ2D_uxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, data, batch_size=32, shuffle=True, seed=42):\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "        self.on_epoch_end()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.data) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        data_batch = np.array([self.data[k] for k in indexes])\n",
        "        #copy of ordered sequences\n",
        "        result = np.copy(data_batch)\n",
        "        #shuffle only the relevant positions for each batch\n",
        "        for i in range(data_batch.shape[0]):\n",
        "          np.random.shuffle(data_batch[i,1:data_batch[i].argmin() - 1])\n",
        "\n",
        "        return data_batch , result\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.arange(len(self.data))\n",
        "        if self.shuffle:\n",
        "            if self.seed is not None:\n",
        "                np.random.seed(self.seed)\n",
        "            np.random.shuffle(self.indexes)"
      ],
      "metadata": {
        "id": "1ZXLkWB6od0R"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a random permutation of training and test set\n",
        "np.random.seed(42)\n",
        "# Shuffle the all data\n",
        "shuffled_indices = np.random.permutation(len(original_data))\n",
        "shuffled_data = original_data[shuffled_indices]"
      ],
      "metadata": {
        "id": "fNo_Jy3N8zHS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split the dataset\n",
        "train_generator = DataGenerator(shuffled_data[:220000])\n",
        "test_generator = DataGenerator(shuffled_data[220000:])"
      ],
      "metadata": {
        "id": "uNlq1Khx1oH2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = test_generator.__getitem__(1)\n",
        "x = detokenizer(x)\n",
        "y = detokenizer(y)\n",
        "\n",
        "for i in range(7):\n",
        "  print(\"original: \", y[i])\n",
        "  print(\"shuffled: \", x[i])\n",
        "  print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qR5xwMOn4E88",
        "outputId": "c15b3995-877e-493d-8f1d-2679bf0c01cc"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original:  <start> ranchers clear large areas of rainforest to become pastures for their cattle <end>\n",
            "shuffled:  <start> become clear rainforest to their for cattle large pastures ranchers areas of <end>\n",
            "\n",
            "\n",
            "original:  <start> some earwigs have stripes on the thorax and abdomen <end>\n",
            "shuffled:  <start> have earwigs the and stripes abdomen on thorax some <end>\n",
            "\n",
            "\n",
            "original:  <start> magnetic manipulation can turn molecules in a liquid into computing such devices <end>\n",
            "shuffled:  <start> a turn into magnetic computing liquid molecules devices in manipulation can such <end>\n",
            "\n",
            "\n",
            "original:  <start> healthy wetlands means cleaner water <comma> reduced flooding and more places for recreation <end>\n",
            "shuffled:  <start> means places <comma> reduced recreation healthy and flooding more cleaner for wetlands water <end>\n",
            "\n",
            "\n",
            "original:  <start> market share is the percent share in sales one company controls in a particular market <end>\n",
            "shuffled:  <start> sales share market controls percent market is particular in in the share a company one <end>\n",
            "\n",
            "\n",
            "original:  <start> face flies spend only a small amount of time on the animal <end>\n",
            "shuffled:  <start> face of amount the spend a small flies only time on animal <end>\n",
            "\n",
            "\n",
            "original:  <start> organic foods are extremely important in prevention and management of cancer <end>\n",
            "shuffled:  <start> management cancer extremely and are of prevention foods in organic important <end>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo8MazCGBTv3"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0NOkuO0CfPo"
      },
      "source": [
        "Let s be the source string and p your prediction. The quality of the results will be measured according to the following metric:\n",
        "\n",
        "1.  look for the longest substring w between s and p\n",
        "2.  compute |w|/max(|s|,|p|)\n",
        "\n",
        "If the match is exact, the score is 1.\n",
        "\n",
        "When computing the score, you should NOT consider the start and end tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-aUrdlXDdVf"
      },
      "source": [
        "The longest common substring can be computed with the SequenceMatcher function of difflib, that allows a simple definition of our metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ulpTRdrF_huh"
      },
      "outputs": [],
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "def score(s,p):\n",
        "  match = SequenceMatcher(None, s, p).find_longest_match()\n",
        "  #print(match.size)\n",
        "  return (match.size/max(len(p),len(s)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB2YfjXNExM-"
      },
      "source": [
        "Let's do an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "h17C8bVjEwur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4da07b5d-d8a9-41d7-bc92-f59a629e551b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your score is  0.5423728813559322\n"
          ]
        }
      ],
      "source": [
        "original = \"at first henry wanted to be friends with the king of france\"\n",
        "generated = \"henry wanted to be friends with king of france at the first\"\n",
        "\n",
        "print(\"your score is \",score(original,generated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BET8GqBvFugR"
      },
      "source": [
        "The score must be computed as an average of at least 3K random examples taken form the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fwo7xj4GBW1"
      },
      "source": [
        "# What to deliver"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6uITuxOGHfJ"
      },
      "source": [
        "You are supposed to deliver a single notebook, suitably commented.\n",
        "The notebook should describe a single model, although you may briefly discuss additional attempts you did.\n",
        "\n",
        "The notebook should contain a full trace of the training.\n",
        "Weights should be made available on request.\n",
        "\n",
        "You must also give a clear assesment of the performance of the model, computed with the metric that has been given to you.\n",
        "\n",
        "# Good work!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}