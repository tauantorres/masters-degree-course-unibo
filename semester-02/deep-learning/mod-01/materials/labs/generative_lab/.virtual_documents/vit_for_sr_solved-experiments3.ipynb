


import os

os.environ["KERAS_BACKEND"] = "tensorflow"  # @param ["tensorflow", "jax", "torch"]

import keras
from keras import layers
from keras import ops

import numpy as np
import matplotlib.pyplot as plt


from setup import *
from generators import * 
from utils import *
from denoising_unet import *


high_path_2010_2019 = "data/wind_speed_italy_CERRA_2010-2019.npy"
high_path_2020 = "data/wind_speed_italy_CERRA_2020.npy"
high_path_2009 = "data/wind_speed_italy_CERRA_2009.npy"
high_path_balcans = "data/wind_speed_BALCANS_CERRA_2009.npy"


low_path_2010_2019 = "data/wind_speed_italy_ERA5_2010-2019.npy"
low_path_2020 = "data/wind_speed_italy_ERA5_2020.npy"
low_path_2009 = "data/wind_speed_italy_ERA5_2009.npy"
low_path_balcans = "data/wind_speed_BALCANS_ERA5_2009.npy"


batch_size = 8


train_generator = DataGeneratorMemmap(high_path_2010_2019,low_path_2010_2019, 
                                      setup.max_high_res, setup.max_low_res, 
                                      setup.dataset_lenght_2010_2019,sequential=False,
                                      batch_size = batch_size, unet = True)

test_generator = DataGeneratorMemmap(high_path_2020,low_path_2020, 
                                      setup.max_high_res, setup.max_low_res, 
                                      setup.dataset_lenght_2020,sequential=False,
                                      batch_size = batch_size, unet = True)

full_test_generator = DataGeneratorMemmap(high_path_2020,low_path_2020, 
                                      setup.max_high_res, setup.max_low_res, 
                                      setup.dataset_lenght_2020,sequential=True,
                                      batch_size = batch_size, unet = True)

full_test_generator2009 = DataGeneratorMemmap(high_path_2009,low_path_2009, 
                                      setup.max_high_res, setup.max_low_res, 
                                      setup.dataset_lenght_2009,sequential=True,
                                      batch_size = batch_size, unet = True)

full_test_generatorBALCANS = DataGeneratorMemmap(high_path_balcans,low_path_balcans, 
                                      setup.max_high_res, setup.max_low_res, 
                                      setup.dataset_lenght_2009,sequential=True,
                                      batch_size = batch_size, unet = True)


input_shape = (256, 256, 4)





learning_rate = 1e-4
weight_decay = 1e-5
num_epochs = 10  # For real training, use num_epochs=100. 10 is a test value
image_size = 256  # We'll resize input images to this size
patch_size = 8  # Size of the patches to be extract from the input images
num_patches = (image_size // patch_size) ** 2
projection_dim = 256
num_heads = 12
transformer_units = [
    projection_dim * 2,
    projection_dim,
]  # Size of the transformer layers
transformer_layers = 6
mlp_head_units = [
    512,
    256,
]  # Size of the dense layers of the final classifier







class Patches(layers.Layer):
    def __init__(self, patch_size):
        super().__init__()
        self.patch_size = patch_size

    def call(self, images):
        input_shape = ops.shape(images)
        batch_size = input_shape[0]
        height = input_shape[1]
        width = input_shape[2]
        channels = input_shape[3]
        num_patches_h = height // self.patch_size
        num_patches_w = width // self.patch_size
        patches = keras.ops.image.extract_patches(images, size=self.patch_size)
        patches = ops.reshape(
            patches,
            (
                batch_size,
                num_patches_h * num_patches_w,
                self.patch_size * self.patch_size * channels,
            ),
        )
        return patches

    def get_config(self):
        config = super().get_config()
        config.update({"patch_size": self.patch_size})
        return config







class PatchEncoder(layers.Layer):
    def __init__(self, num_patches, projection_dim):
        super().__init__()
        self.num_patches = num_patches
        self.projection = layers.Dense(units=projection_dim)
        self.position_embedding = layers.Embedding(
            input_dim=num_patches, output_dim=projection_dim
        )

    def call(self, patch):
        positions = ops.expand_dims(
            ops.arange(start=0, stop=self.num_patches, step=1), axis=0
        )
        projected_patches = self.projection(patch)
        encoded = projected_patches + self.position_embedding(positions)
        return encoded

    def get_config(self):
        config = super().get_config()
        config.update({"num_patches": self.num_patches})
        return config



# only used in postprocessing 
def ResidualBlock(width):
            def apply(x):
                input_width = x.shape[3]
                if input_width == width:
                    residual = x
                else:
                    residual = layers.Conv2D(width, kernel_size=1)(x)
                #x = layers.BatchNormalization(center=False, scale=False)(x)
                x = layers.LayerNormalization(axis=-1,center=True, scale=True)(x)
                x = layers.Conv2D(
                    width, kernel_size=3, padding="same", activation=keras.activations.swish
                )(x)
                x = layers.Conv2D(width, kernel_size=3, padding="same")(x)
                x = layers.Add()([x, residual])
                return x
        
            return apply



def mlp(x, hidden_units, dropout_rate):
    for units in hidden_units:
        x = layers.Dense(units, activation=keras.activations.gelu)(x)
        #x = layers.Dropout(dropout_rate)(x)
    return x


def create_vit():
    inputs = keras.Input(shape=input_shape)
    # Create patches
    patches = Patches(patch_size)(inputs)
    # Encode patches.
    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)

    # Create multiple layers of the Transformer block.
    for _ in range(transformer_layers):
        # Layer normalization 1.
        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
        # Create a multi-head attention layer.
        attention_output = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=projection_dim, dropout=0.1
        )(x1, x1)
        # Skip connection 1.
        x2 = layers.Add()([attention_output, encoded_patches])
        # Layer normalization 2.
        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)
        # MLP.
        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)
        # Skip connection 2.
        encoded_patches = layers.Add()([x3, x2])

    # Create a [batch_size, projection_dim] tensor.
    out = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
    # Spatial reshape of the patches
    out = layers.Reshape((32, 32, 256))(out)
    # Residual block
    out = ResidualBlock(256)(out)
    # Upsample
    out = layers.Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same', activation='relu')(out)
    out = ResidualBlock(128)(out)
    # Upsample while reducing channel size
    out = layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', activation='relu')(out)
    out = ResidualBlock(64)(out)
    out = layers.Conv2DTranspose(1, (3, 3), strides=(2, 2), padding='same', activation='relu')(out)

    model = keras.Model(inputs=inputs, outputs=out)
    return model



from tensorflow.keras.callbacks import Callback
import os

class CustomSaveModelCallback(Callback):
    def __init__(self, save_freq = 20, save_path='./vit_weights_exp3/'):
        super(CustomSaveModelCallback, self).__init__()
        self.save_freq = save_freq
        self.save_path = save_path
        os.makedirs(save_path, exist_ok=True)

    def on_epoch_end(self, epoch, logs=None):
        if (epoch + 1) % self.save_freq == 0:
            filename = f'weights_epoch_{epoch + 1}.weights.h5'
            file_path = os.path.join(self.save_path, filename)
            self.model.save_weights(file_path)
            print(f'Saved weights at epoch {epoch + 1} to {file_path}')


model = create_vit()

optimizer = keras.optimizers.AdamW(
    learning_rate=learning_rate, weight_decay=weight_decay
)

model.compile(
    optimizer=optimizer,
    loss=keras.losses.MeanSquaredError(),
)


num_epochs = 1



model.summary(positions = [0.3, 0.6, 0.80, 1.])





#model.load_weights("vit_weights_exp2/weights_epoch_220.weights.h5")


save_call = CustomSaveModelCallback(20)
history = model.fit(
    train_generator,
    #initial_epoch = 100, 
    #validation_data = valid_generator, 
    epochs=230,
    steps_per_epoch=500,
    #batch_size=32,
    callbacks=[
        save_call
    ],
)


loss = history.history['loss']


plt.plot(loss[50:])


test = model.predict(train_generator.__getitem__(1)[0])


plt.imshow(test[0])


def experiment_espcn(generator, n_iter=100):
    #define final mse array 
    #raw = np.zeros((91,32,256,256))
    #bilinear = np.zeros((91,32,256,256))
    mses = np.zeros(n_iter)
    mses_baseline = np.zeros(n_iter)
    
    ssims = np.zeros(n_iter)
    psnrs = np.zeros(n_iter)
    
    ssims_baseline = np.zeros(n_iter)
    psnrs_baseline = np.zeros(n_iter)
    for i in range(n_iter):
        if(i%5 == 0):
            print(i)
        #select a random batch in the test set  
        sampin,sampout = generator.__getitem__(i)
        
        tmp = model.predict(sampin)
        
        sampin2 = np.zeros((sampin.shape[0],256,256,4))
        for l in range(sampin.shape[0]):
            for m in range(4):
                sampin2[l,:,:,m] = cv2.resize(sampin[l,:,:,m],(256,256),interpolation=cv2.INTER_LINEAR)
        sampin = sampin2
        
        mse = np.mean( ((sampout-np.squeeze(tmp))**2))
        ssims[i] = batch_ssim(sampout,np.squeeze(tmp))
        psnrs[i] = batch_psnr(sampout,np.squeeze(tmp))
        
        mse_baseline = np.mean( ((sampin[:,:,:,-2]-sampout)**2))
        ssims_baseline[i] = batch_ssim(sampin[:,:,:,-2],sampout)
        psnrs_baseline[i] = batch_psnr(sampin[:,:,:,-2],sampout)
        #print(mse.shape)
        # add 3 relevant meteric values to array 
        mses_baseline[i] = mse_baseline
        mses[i] = mse
        #raw[i] = np.squeeze(tmp)
        #bilinear[i] = sampin[:,:,:,-2]
    # return average of all mses
    return mses, mses_baseline, psnrs, psnrs_baseline, ssims, ssims_baseline#, raw, bilinear


full_test_generator = DataGeneratorMemmap(high_path_2020,low_path_2020, 
                                      setup.max_high_res, setup.max_low_res, 
                                      setup.dataset_lenght_2020,sequential=True,
                                      batch_size = 32, unet = True)

full_test_generator2009 = DataGeneratorMemmap(high_path_2009,low_path_2009, 
                                      setup.max_high_res, setup.max_low_res, 
                                      setup.dataset_lenght_2009,sequential=True,
                                      batch_size = 32, unet = True)


import time
full_test_generator.counter_reset()
start_time = time.time()  # Capture the start time
mses, mses_baseline, psnrs, psnrs_baseline, ssims, ssims_baseline = experiment_espcn(full_test_generator,91)
end_time = time.time()  # Capture the end time
execution_time = end_time - start_time 


import time
full_test_generator.counter_reset()
start_time = time.time()  # Capture the start time
mses, mses_baseline, psnrs, psnrs_baseline, ssims, ssims_baseline = experiment_espcn(full_test_generator2009,91)
end_time = time.time()  # Capture the end time
execution_time = end_time - start_time 


#2009
print("ssim:", ssims.mean(), "  ssim baseline:", ssims_baseline.mean())
print("psnrs:", psnrs.mean(), "  psnrs_baseline:", psnrs_baseline.mean())
print("mse:", mses.mean(), "  mse baseline:", mses_baseline.mean())


#2020
print("ssim:", ssims.mean(), "  ssim baseline:", ssims_baseline.mean())
print("psnrs:", psnrs.mean(), "  psnrs_baseline:", psnrs_baseline.mean())
print("mse:", mses.mean(), "  mse baseline:", mses_baseline.mean())


def print_results(a,b,c):
    
    for i in range(a.shape[0]):
        fig, axes = plt.subplots(1, 3, figsize=(15, 5.5))
        
        ax = axes[0]
        ax.imshow(a[i,:,:,-1], cmap='viridis')  # Use appropriate colormap for your data
        ax.axis('off')
        ax.set_title("last low res")
        
        ax = axes[1]
        ax.imshow(c[i,:,:], cmap='viridis')  # Use appropriate colormap for your data
        ax.axis('off')
        ax.set_title("predicted")

        ax = axes[2]
        ax.imshow(b[i,:,:], cmap='viridis')  # Use appropriate colormap for your data
        ax.axis('off')
        ax.set_title("ground truth")       

        plt.tight_layout()
        #plt.savefig("unet" + str(i) + ".png")
        plt.show()
        


sampin,sampout = train_generator.__getitem__(1)
tmp = model.predict(sampin)

print_results(sampin,sampout,tmp)



