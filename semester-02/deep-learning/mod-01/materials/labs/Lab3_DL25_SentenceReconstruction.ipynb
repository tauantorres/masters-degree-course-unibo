{"cells":[{"cell_type":"markdown","source":["#Deep Learning lab 3\n","\n","Sentence Reordering Task.\n","Text input:\n","Text output:\n","original  (truth):<start> orcas use echolocation to talk to each other and hunt <end>\n","shuffled (input): <start> talk echolocation and to to use each hunt orcas other <end>\n","generate (output):"],"metadata":{"id":"5Y7nOqe_rAHP"}},{"cell_type":"code","execution_count":28,"metadata":{"collapsed":true,"executionInfo":{"elapsed":2406,"status":"ok","timestamp":1746620517078,"user":{"displayName":"Salvatore Fiorilla","userId":"08470273097812624739"},"user_tz":-120},"id":"nXyepy59qJHd"},"outputs":[],"source":["!pip install datasets > /dev/null #> /dev/null to print only the errors"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":54996,"status":"ok","timestamp":1746612465414,"user":{"displayName":"Salvatore Fiorilla","userId":"08470273097812624739"},"user_tz":-120},"id":"GFv55PKUv_dL","outputId":"e14fe0be-111d-4502-ed56-29396b8dc564"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset types: <class 'datasets.arrow_dataset.Dataset'>\n","Dataset Structure:\n"," Dataset({\n","    features: ['source', 'term', 'quantifier_frequency', 'quantifier_number', 'generic_sentence', 'score'],\n","    num_rows: 1020868\n","})\n"]}],"source":["import keras\n","import tensorflow as tf\n","import numpy as np\n","\n","from datasets import load_dataset\n","from keras.layers import TextVectorization\n","\n","#Hyperparamenters\n","VOCAB_SIZE = 10000\n","MAX_SEQ_LEN=28\n","MIN_SEQ_LEN=9\n","\n","ds = load_dataset('generics_kb',trust_remote_code=True)['train']\n","print(\"Dataset types:\",type(ds))\n","print(\"Dataset Structure:\\n\",ds)\n","ds = ds.filter(lambda row: len(row['generic_sentence'].split(\" \"))>=MIN_SEQ_LEN )\n","\n","corpus = ['<start> ' + row['generic_sentence'].replace(\",\",\" <comma>\") + ' <end>' for row in ds ]\n","corpus = np.array(corpus)\n","tokenizer=TextVectorization( max_tokens=VOCAB_SIZE, standardize=\"lower_and_strip_punctuation\")\n","tokenizer.adapt(corpus)\n","sentences = tokenizer( corpus ).numpy()\n","\n","mask = np.sum( (sentences==1) , axis=1) >= 1  #check if <start> appears more than once for each sentence.\n","original_data = np.delete( sentences, mask , axis=0)\n","original_data = [sen for sen in original_data if not(1 in sen) and len(sen) <= MAX_SEQ_LEN]\n","\n","#original_data = [sen for sen in tokenizer(corpus).numpy() if not(1 in sen) and len(sen)>4 and len(sen)<= 32]\n","\n"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":114,"status":"ok","timestamp":1746621474204,"user":{"displayName":"Salvatore Fiorilla","userId":"08470273097812624739"},"user_tz":-120},"id":"OakC8UPU64C5","outputId":"38c5ced2-abdf-47ff-ffa8-58f6ff6de1e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total length: 241236\n","Train set length:  220000\n","Test set length:  21236\n","\n","Sample 1: from shuffled to original\n","Tokenized input: 3 2743 78 11 83 10 2780 7 20 219 7 8 5 713 6 2 0 0 0 0 0 0 0 0 0 0 0 0\n","Tokenized output: 3 2780 8 7 11 78 83 7 10 219 5 2743 6 20 713 2 0 0 0 0 0 0 0 0 0 0 0 0\n","Text input: <start> variables large in part a sociology <comma> their study <comma> is of relationships and <end>\n","Text output: <start> sociology is <comma> in large part <comma> a study of variables and their relationships <end>\n","\n","Sample 2: from shuffled to original\n","Tokenized input: 3 184 467 10 15 11 471 8 7282 674 678 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","Tokenized output: 3 184 8 10 467 7282 15 471 11 678 674 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","Text input: <start> stress popular a for in illness is explanation america north <end>\n","Text output: <start> stress is a popular explanation for illness in north america <end>\n","\n","Sample 3: from shuffled to original\n","Tokenized input: 3 11 2397 109 4 77 51 99 64 154 38 62 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","Tokenized output: 3 77 109 11 4 64 51 38 62 2397 99 5 154 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","Text input: <start> in divide occurs the cancer body out human control when cells of <end>\n","Text output: <start> cancer occurs in the human body when cells divide out of control <end>\n"]}],"source":["# Initial Shuffle of the original_data\n","shuffled_indices = np.random.permutation(len(original_data))\n","original_data = np.array(original_data)[shuffled_indices]\n","\n","class TextDetokenizer:\n","        def __init__(self, vectorize_layer):\n","          self.vectorize_layer = vectorize_layer\n","          vocab = self.vectorize_layer.get_vocabulary()\n","          self.index_to_word = {index: word for index, word in enumerate(vocab)}\n","\n","        def __detokenize_tokens(self, tokens):\n","\n","          def check_token(t):\n","              if t==3:\n","                  s=\"<start>\"\n","              elif t==2:\n","                  s=\"<end>\"\n","              elif t==7:\n","                  s=\"<comma>\"\n","              else:\n","                  s=self.index_to_word.get(t, '[UNK]')\n","              return s\n","\n","          return ' '.join([ check_token(token) for token in tokens if token != 0])\n","\n","        def __call__(self, batch_tokens):\n","             return [self.__detokenize_tokens(tokens) for tokens in batch_tokens]\n","\n","\n","from keras.utils import Sequence\n","class DataGenerator(Sequence):\n","        def __init__(self, data, batch_size=32, shuffle=True, seed=None):\n","\n","            self.data = data\n","            self.batch_size = batch_size\n","            self.shuffle = shuffle\n","            self.seed = seed\n","            self.on_epoch_end()\n","\n","        def __iter__(self):\n","          for i in range(len(self)):\n","            yield self[i]\n","\n","        def __len__(self):\n","            return int(np.floor(len(self.data) / self.batch_size))\n","\n","        def __getitem__(self, index):\n","            indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","            data_batch = np.array([self.data[k] for k in indexes])\n","            #copy of ordered sequences\n","            result = np.copy(data_batch)\n","            #shuffle only the relevant positions for each batch\n","            for i in range(data_batch.shape[0]):\n","                np.random.shuffle(data_batch[i,1:data_batch[i].argmin() - 1])\n","\n","            return data_batch,result\n","\n","        def on_epoch_end(self):\n","            self.indexes = np.arange(len(self.data))\n","            if self.shuffle:\n","                if self.seed is not None:\n","                    np.random.seed(self.seed)\n","                np.random.shuffle(self.indexes)\n","\n","detokenizer = TextDetokenizer( tokenizer )\n","\n","#data split\n","total_len = len(original_data)\n","last_train_idx = 220000\n","\n","print(\"Total length:\", total_len)\n","print(\"Train set length: \",last_train_idx)\n","print(\"Test set length: \",total_len - last_train_idx)\n","\n","train_generator = DataGenerator(original_data[:last_train_idx])\n","test_generator = DataGenerator(original_data[last_train_idx:])\n","\n","detokenizer = TextDetokenizer(tokenizer)\n","batch_x, batch_y = train_generator[0]\n","\n","detokenized_x = detokenizer(batch_x)\n","detokenized_y = detokenizer(batch_y)\n","\n","\n","for i in range(3):\n","    print(f\"\\nSample {i+1}: from shuffled to original\")\n","    # Token (sequenza di numeri)\n","    print(f\"Tokenized input:\", ' '.join(map(str, batch_x[i])))\n","    print(f\"Tokenized output:\", ' '.join(map(str, batch_y[i])))\n","    # Detokenized (frase decodificata)\n","    print(f\"Text input:\", detokenized_x[i])\n","    print(f\"Text output:\", detokenized_y[i])\n"]},{"cell_type":"markdown","metadata":{"id":"yrYjQCU_kbrx"},"source":["Let's work on the model.\n","We'll training and test a transformer"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5140,"status":"ok","timestamp":1746612470704,"user":{"displayName":"Salvatore Fiorilla","userId":"08470273097812624739"},"user_tz":-120},"id":"BzuagOBPnJtD","outputId":"75d3d19d-9c8d-42dd-a35a-cb5b1920f069"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["!pip install --upgrade keras-hub keras > /dev/null\n","\n","import os\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":691},"executionInfo":{"elapsed":419,"status":"ok","timestamp":1746621192787,"user":{"displayName":"Salvatore Fiorilla","userId":"08470273097812624739"},"user_tz":-120},"id":"mJKIkOwsw8-F","outputId":"14f45cea-b129-405f-aa02-c4fc18a80d64"},"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"Encoder_Model\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Encoder_Model\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ encoder_input (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ encoder_embedding (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m5,120,000\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_encoder_18          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m1,167,976\u001b[0m │\n","│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_encoder_19          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m1,167,976\u001b[0m │\n","│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_encoder_20          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m1,167,976\u001b[0m │\n","│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ encoder_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ encoder_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120,000</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_encoder_18          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,167,976</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_encoder_19          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,167,976</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_encoder_20          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,167,976</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,623,928\u001b[0m (32.90 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,623,928</span> (32.90 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,623,928\u001b[0m (32.90 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,623,928</span> (32.90 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"Decoder_Model\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Decoder_Model\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m5,120,000\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n","│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ encoder_output      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ transformer_decode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │  \u001b[38;5;34m2,203,216\u001b[0m │ decoder_embeddin… │\n","│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ encoder_output[\u001b[38;5;34m0\u001b[0m… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_10 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m10000\u001b[0m) │  \u001b[38;5;34m5,130,000\u001b[0m │ transformer_deco… │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120,000</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ encoder_output      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ transformer_decode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,203,216</span> │ decoder_embeddin… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ encoder_output[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,130,000</span> │ transformer_deco… │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,453,216\u001b[0m (47.51 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,453,216</span> (47.51 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,453,216\u001b[0m (47.51 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,453,216</span> (47.51 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}],"source":["from keras import Model\n","from keras.layers import Input, Embedding, Dense\n","from keras_hub.layers import TransformerEncoder, TransformerDecoder\n","import keras\n","\n","# Hyperparameters\n","vocab_size = VOCAB_SIZE\n","sequence_length=28\n","\n","intermediate_dim = 128\n","num_heads=12\n","embedding_dim=512\n","num_layers=3\n","\n","# 3L config is\n","# intermediate_dim = 64\n","# num_heads=8\n","# embedding_dim=128\n","# num_layers=3\n","\n","# 1L config is\n","# intermediate_dim = 64\n","# num_heads=8\n","# embedding_dim=64\n","# num_layers=1\n","\n","\n","\n","# Modifichiamo le funzioni per incorporare gli embedding layer\n","def create_encoder_model():\n","    encoder_inputs = Input(shape=(sequence_length,), name=\"encoder_input\")\n","\n","    # Embedding layer per l'encoder\n","    embedding = Embedding(\n","                 input_dim=vocab_size,\n","                 output_dim=embedding_dim,\n","                 name=\"encoder_embedding\"\n","                )(encoder_inputs)\n","\n","    # Transformer encoder blocks\n","    encoder_outputs = embedding\n","    for _ in range(num_layers):\n","        encoder_outputs=TransformerEncoder(\n","            intermediate_dim=intermediate_dim,\n","            num_heads=num_heads\n","            )(encoder_outputs)\n","\n","    return Model(inputs=encoder_inputs, outputs=encoder_outputs, name=\"Encoder_Model\")\n","\n","\n","def create_decoder_model():\n","    seqs_inputs = Input(shape=(sequence_length-1,), name=\"decoder_input\")\n","    embedding = Embedding(\n","                  input_dim=vocab_size,\n","                  output_dim=embedding_dim,\n","                  name=\"decoder_embedding\"\n","                )(seqs_inputs)\n","\n","    enc_outputs = Input(shape=(sequence_length, embedding_dim), name=\"encoder_output\")\n","\n","    # Transformer dencoder layers\n","    denc_outputs = embedding\n","\n","    for _ in range(num_layers):\n","        dec_outputs=TransformerDecoder(intermediate_dim=intermediate_dim,num_heads=num_heads)(embedding, enc_outputs)\n","\n","    # Proiezione finale sul vocabolario\n","    outputs = Dense(vocab_size, activation='softmax')(dec_outputs)\n","\n","    return Model(inputs=[seqs_inputs, enc_outputs], outputs=outputs, name=\"Decoder_Model\")\n","\n","\n","\n","# Creazione dei modelli separati\n","encoder_model = create_encoder_model()\n","decoder_model = create_decoder_model()\n","\n","\n","encoder_model.summary()\n","print()\n","decoder_model.summary()\n"]},{"cell_type":"markdown","metadata":{"id":"nN4khT3O_UBt"},"source":["We have to understand the training process first.\n","\n","What the model predict is the next token of a sequence.\n","\n","The output of the decoder is the vocaboulary-shape vector of probabilities."]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1746621489292,"user":{"displayName":"Salvatore Fiorilla","userId":"08470273097812624739"},"user_tz":-120},"id":"rIhCao-2IVYW","outputId":"5a4920c1-54b4-423e-b351-f25a4be3e433"},"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"SLM_Model\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"SLM_Model\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ Encoder_Model (\u001b[38;5;33mFunctional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m8,623,928\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ Decoder_Model (\u001b[38;5;33mFunctional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m10000\u001b[0m)      │    \u001b[38;5;34m12,453,216\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ Encoder_Model (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,623,928</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ Decoder_Model (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,453,216</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,077,144\u001b[0m (80.40 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,077,144</span> (80.40 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m21,077,144\u001b[0m (80.40 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,077,144</span> (80.40 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}],"source":["# Costum train loop of a Combined Keras Model\n","class SLM(keras.Model):\n","    def __init__(self, encoder_model, decoder_model, **kwargs):\n","        super(SLM, self).__init__(name=\"SLM_Model\",**kwargs)\n","        self.encoder = encoder_model\n","        self.decoder = decoder_model\n","        self.loss_tracker = keras.metrics.Mean(name='loss')\n","\n","    def save_path(self,name):\n","      return f\"/content/drive/MyDrive/DLlab_2425/sentence_reordering_{name}.weights.h5\"\n","\n","    def load_weights(self,):\n","        self.encoder.load_weights(self.save_path(\"enc\"))\n","        self.decoder.load_weights(self.save_path(\"dec\"))\n","        return\n","\n","    def save_weights(self, ):\n","        self.encoder.save_weights(self.save_path(\"enc\"))\n","        self.decoder.save_weights(self.save_path(\"dec\"))\n","        return\n","\n","    def train_step(self, data):\n","        batch_x, batch_y = data\n","\n","        with tf.GradientTape() as tape:\n","\n","            # Encoder forward pass\n","            encoder_outputs = self.encoder(batch_x, training=True)\n","\n","            # Right shift of the target sentence as input\n","            decoder_inputs = batch_y[:,:-1]\n","\n","            # Decoder forward pass\n","            predictions = self.decoder([decoder_inputs, encoder_outputs], training=True)\n","\n","            # Compute the loss\n","            loss = self.compiled_loss(batch_y[:,1:], predictions)\n","\n","        # Compute the gradients\n","        trainable_vars = self.encoder.trainable_variables + self.decoder.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","\n","        # Update the weigths\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","\n","        # Update the metrics\n","        self.loss_tracker.update_state(loss)\n","        return {\"loss\": self.loss_tracker.result()}\n","\n","\n","    def call(self, batch_x, training=False):\n","      return self.encoder(batch_x, training=training)\n","\n","    def generate(self, batch_of_shuffled_tokens, training=False, max_length=sequence_length):\n","        contexts = self.encoder(batch_of_shuffled_tokens, training=training)\n","        predictions = []\n","        for i, ctx in enumerate(contexts):\n","            ctx = tf.expand_dims(ctx, 0)\n","            end_token = 2\n","            start_token= tf.constant([[3]]) #start_token value\n","            dummy_paddings = tf.zeros((1, max_length - 2), dtype=tf.int32) #add paddings to fill the sentence\n","\n","            output_sequence = tf.concat([start_token, dummy_paddings ], axis=1)\n","\n","            for step in range(max_length-2):\n","              preds = self.decoder([output_sequence, ctx], training=training)\n","              #next token is the last most likely index of the vocabulary\n","              next_token = tf.argmax(preds[:, step, :] , axis=-1, output_type=tf.int32)\n","\n","              #output_sequence[step+1]= next_token\n","              output_sequence = tf.tensor_scatter_nd_update(\n","                output_sequence, [[0, step + 1]], [next_token[0]]  # Use step + 1 for correct index\n","              )\n","\n","              if next_token[0].numpy() == end_token:\n","                break\n","\n","            predictions.append(output_sequence)\n","\n","        results = tf.concat(predictions, axis=0).numpy()\n","        return results\n","\n","    def generate_vec(self, batch_of_shuffled_tokens, training=False, max_length=sequence_length):\n","        contexts = self.encoder(batch_of_shuffled_tokens, training=training)\n","\n","        batch_size = tf.shape(contexts)[0]\n","        end_token = 2\n","\n","        # we initialize start+puddings for each sequence for each context\n","        start_tokens = tf.fill([batch_size, 1], 3)  # start_token value\n","        dummy_paddings = tf.zeros([batch_size, max_length - 2], dtype=tf.int32)\n","        output_sequences = tf.concat([start_tokens, dummy_paddings], axis=1)\n","\n","        # tracking of active_sequences\n","        active_sequences = tf.ones([batch_size], dtype=tf.bool)\n","\n","        def loop_body(step, output_sequences, active_sequences):\n","            preds = self.decoder([output_sequences, contexts], training=training)\n","            next_tokens = tf.argmax(preds[:, step, :], axis=-1, output_type=tf.int32)\n","            indices = tf.stack([tf.range(batch_size), tf.fill([batch_size], step + 1)], axis=1)\n","            output_sequences = tf.tensor_scatter_nd_update(\n","                output_sequences, indices, next_tokens\n","            )\n","            is_end_token = tf.equal(next_tokens, end_token)\n","            active_sequences = active_sequences & ~is_end_token\n","            return step + 1, output_sequences, active_sequences\n","\n","\n","        # Use tf.while_loop instead of Python for loop\n","        _, output_sequences, _ = tf.while_loop(\n","            cond= lambda step, *_: step < max_length - 2 and tf.reduce_any(active_sequences),\n","            body= loop_body,\n","            loop_vars=[0, output_sequences, active_sequences]\n","        )\n","\n","        return output_sequences.numpy()\n","\n","    @property\n","    def metrics(self):\n","        return [self.loss_tracker]\n","\n","\n","\n","\n","# Creazione e compilazione del modello combinato\n","model = SLM(encoder_model, decoder_model)\n","\n","\n","from keras.losses import sparse_categorical_crossentropy\n","\n","def custom_vocab_sparsecatcrossentropy(y_true, y_pred):\n","    loss = sparse_categorical_crossentropy(y_true, y_pred, from_logits=False)\n","    mask = tf.cast(tf.not_equal(y_true, 0), dtype=tf.float32)\n","    return tf.reduce_sum(loss * mask) / tf.reduce_sum(mask)\n","\n","model.compile(\n","    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n","    loss=custom_vocab_sparsecatcrossentropy\n",")\n","model.summary()\n"]},{"cell_type":"code","source":["from difflib import SequenceMatcher\n","class ScoreCallback(keras.callbacks.Callback):\n","    def __init__(self, epoch_interval=None, valid_set=test_generator):\n","        self.epoch_interval = epoch_interval\n","        self.valid_set = valid_set\n","\n","    def compute_score_on_sentence(self, s,p):\n","        match = SequenceMatcher(None, s, p).find_longest_match()\n","        return (match.size/max(len(p),len(s)))\n","\n","    def score_fun(self, sentences,predictions):\n","      scores = map(lambda s, p: self.compute_score_on_sentence(s, p), sentences, predictions)\n","      return np.mean(list(scores))\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        #if self.epoch_interval and epoch % self.epoch_interval == 0:\n","        if self.epoch_interval and (epoch + 1) % self.epoch_interval == 0:\n","            cum_scores=[]\n","            self.model.save_weights()\n","            print(\"\\nSaved models, computing scores..\")\n","\n","            #take a random batch form valid_set\n","            batch_x, batch_y = self.valid_set[np.random.randint(len(self.valid_set))]\n","\n","            # for batch_x, batch_y in self.valid_set:\n","            preds  = self.model.generate(batch_x)\n","            generated = detokenizer(preds)\n","            originals = detokenizer(batch_y)\n","\n","            print(\"\\nOriginal:  \", originals[0])\n","            print(\"Shuffled:  \", detokenizer(batch_x[None, 0])[0])\n","            print(\"Generated: \", generated[0])\n","            print(\"Got score: \", self.compute_score_on_sentence(originals[0], generated[0]))\n","\n","            score = self.score_fun(originals, originals)\n","            print(f\"Batch score: {np.mean(np.array(score))}\\n\",)\n","\n","\n","\n","early_stopping = keras.callbacks.EarlyStopping(\n","    monitor=\"loss\",\n","    restore_best_weights=True,\n","    start_from_epoch=0,\n","    patience=3\n",")\n","\n","\n","model.fit(\n","    train_generator,\n","    epochs=100,\n","    callbacks=[ScoreCallback(10),early_stopping]\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I76Xl1s0tKPU","outputId":"62cdfa4c-d516-4a77-b8c1-8663f892279b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","\u001b[1m6875/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 23ms/step - loss: 4.4127\n","Epoch 2/100\n","\u001b[1m6875/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 23ms/step - loss: 3.9752\n","Epoch 3/100\n","\u001b[1m6875/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 23ms/step - loss: 3.7469\n","Epoch 4/100\n","\u001b[1m6875/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 23ms/step - loss: 3.5677\n","Epoch 5/100\n","\u001b[1m6875/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 23ms/step - loss: 3.4109\n","Epoch 6/100\n","\u001b[1m6875/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 23ms/step - loss: 3.2685\n","Epoch 7/100\n","\u001b[1m6875/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 23ms/step - loss: 3.1442\n","Epoch 8/100\n","\u001b[1m6875/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 23ms/step - loss: 3.0306\n","Epoch 9/100\n","\u001b[1m6875/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 23ms/step - loss: 2.9340\n","Epoch 10/100\n","\u001b[1m6874/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.8446\n","Saved models, computing scores..\n","\n","Original:   <start> criminal justice is both a structure and a process <end>\n","Shuffled:   <start> a structure and process both justice criminal is a <end>\n","Generated:  <start> some people have a lot of trouble getting around <end>\n","Got score:  0.125\n","Batch score: 1.0\n","\n","\u001b[1m6875/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 24ms/step - loss: 2.8446\n","Epoch 11/100\n","\u001b[1m6875/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 23ms/step - loss: 2.7699\n","Epoch 12/100\n","\u001b[1m6875/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 23ms/step - loss: 2.7070\n","Epoch 13/100\n","\u001b[1m6875/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 23ms/step - loss: 2.6483\n","Epoch 14/100\n","\u001b[1m6875/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 23ms/step - loss: 2.6011\n","Epoch 15/100\n","\u001b[1m6875/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 23ms/step - loss: 2.5582\n","Epoch 16/100\n","\u001b[1m6875/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 23ms/step - loss: 2.5223\n","Epoch 17/100\n","\u001b[1m6875/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 23ms/step - loss: 2.4886\n","Epoch 18/100\n","\u001b[1m6875/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 23ms/step - loss: 2.4562\n","Epoch 19/100\n","\u001b[1m6875/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 23ms/step - loss: 2.4307\n","Epoch 20/100\n","\u001b[1m6874/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.4067\n","Saved models, computing scores..\n","\n","Original:   <start> progress means that all people of the earth have a truly equal opportunity to achieve <end>\n","Shuffled:   <start> truly have a opportunity achieve the people equal all earth means of to that progress <end>\n","Generated:  <start> some people are allergic to the pollen <comma> which is carried on the wind and the wind <end>\n","Got score:  0.0784313725490196\n","Batch score: 1.0\n","\n","\u001b[1m6875/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 26ms/step - loss: 2.4067\n","Epoch 21/100\n","\u001b[1m6875/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 23ms/step - loss: 2.3850\n","Epoch 22/100\n","\u001b[1m6875/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 23ms/step - loss: 2.3655\n","Epoch 23/100\n","\u001b[1m6875/6875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 23ms/step - loss: 2.3454\n","Epoch 24/100\n","\u001b[1m3976/6875\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 23ms/step - loss: 2.2786"]}]},{"cell_type":"code","source":["\n","\n","\n","model.save_weights()"],"metadata":{"id":"n7gK5MomzZEl","executionInfo":{"status":"ok","timestamp":1746612472533,"user_tz":-120,"elapsed":1155,"user":{"displayName":"Salvatore Fiorilla","userId":"08470273097812624739"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QjmYQqDLY4hR","outputId":"00d0fc42-9cd5-424e-e6b4-711fee285480","executionInfo":{"status":"ok","timestamp":1746612475784,"user_tz":-120,"elapsed":3249,"user":{"displayName":"Salvatore Fiorilla","userId":"08470273097812624739"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Processing batches: 100%|██████████| 663/663 [00:03<00:00, 204.35batch/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Baseline Score:  0.16967992856444322\n","Acceptable Score:  0.19412633590159906\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["from difflib import SequenceMatcher\n","\n","#Baseline test\n","def compute_score_on_sentence(s,p):\n","    matches = SequenceMatcher(None, s, p).find_longest_match()\n","    return (matches.size/max(len(p),len(s)))\n","\n","from difflib import SequenceMatcher\n","def compute_score_on_batch(seq,pred):\n","    scores = map(lambda s, p: compute_score_on_sentence(s, p), seq, pred)\n","    return np.mean(list(scores))\n","\n","\n","#compute the baseline\n","cum_scores=[]\n","from tqdm import tqdm\n","\n","total_iters = len(test_generator)\n","for i, (batch_x, batch_y) in enumerate(tqdm(test_generator, desc=\"Processing batches\", unit=\"batch\")):\n","    #completely random shuffle\n","    generated = detokenizer(batch_x)\n","    originals = detokenizer(batch_y)\n","    score_value = compute_score_on_batch(originals, generated)\n","    cum_scores.append(score_value)\n","\n","\n","cum_scores = np.array(cum_scores)\n","baseline = np.mean(cum_scores)\n","print(\"\\nBaseline Score: \",baseline)\n","print(\"Acceptable Score: \",baseline + 3 * np.std(cum_scores))\n"]},{"cell_type":"code","source":["#final test\n","\n","VERBOSE=False\n","model.load_weights()\n","cum_scores=[]\n","for i, (batch_x, batch_y) in enumerate(tqdm(test_generator, desc=\"Final Test\", unit=\"batch\")):\n","    pred_tokens  = model.generate_vec(batch_x)\n","    generated = detokenizer(pred_tokens)\n","    originals = detokenizer(batch_y)\n","    score_value = compute_score_on_batch(originals, generated)\n","    cum_scores.append(score_value)\n","    if VERBOSE:\n","      print(\"\\nOriginal:  \", originals[0])\n","      print(\"Shuffled: \", detokenizer(batch_x[None, 0])[0])\n","      print(\"Generated: \", generated[0])\n","      print(\"Got score: \",score_value)\n","\n","print(\"Matching Score: \",np.mean(np.array(cum_scores)))"],"metadata":{"id":"AAfu7Qwaz-Co"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rz2dUgMG6dn0","executionInfo":{"status":"ok","timestamp":1746620514668,"user_tz":-120,"elapsed":595891,"user":{"displayName":"Salvatore Fiorilla","userId":"08470273097812624739"}},"outputId":"212ba75c-cfc5-492f-e137-e17c12deeb0e"},"outputs":[{"output_type":"stream","name":"stderr","text":["Final Test: 100%|██████████| 663/663 [09:55<00:00,  1.11batch/s]"]},{"output_type":"stream","name":"stdout","text":["Matching Score:  0.23170012875095977\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["#Previous model final test\n","VERBOSE=False\n","model.load_weights()\n","cum_scores=[]\n","for i, (batch_x, batch_y) in enumerate(tqdm(test_generator, desc=\"Final Test\", unit=\"batch\")):\n","    pred_tokens  = model.generate_vec(batch_x)\n","    generated = detokenizer(pred_tokens)\n","    originals = detokenizer(batch_y)\n","    score_value = compute_score_on_batch(originals, generated)\n","    cum_scores.append(score_value)\n","    if VERBOSE:\n","      print(\"\\nOriginal:  \", originals[0])\n","      print(\"Shuffled: \", detokenizer(batch_x[None, 0])[0])\n","      print(\"Generated: \", generated[0])\n","      print(\"Got score: \",score_value)\n","\n","print(\"Matching Score: \",np.mean(np.array(cum_scores)))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"mount_file_id":"1NjrTHAfpRH6zF8joWAE1TCbOy-kAGzUA","authorship_tag":"ABX9TyMeFgQAyqyvobaB2zcqy3K3"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}